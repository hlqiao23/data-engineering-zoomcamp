{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hlqiao23/data-engineering-zoomcamp/blob/main/05-batch/code/spark_learning_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Refernec Notes:\n",
        "https://github.com/ziritrion/dataeng-zoomcamp/blob/main/notes/5_batch_processing.md"
      ],
      "metadata": {
        "id": "-r7qP46qANKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites\n",
        "Sign up to https://ngrok.com/ to be able to reach Spark UI"
      ],
      "metadata": {
        "id": "EmXS59l00s7l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSaYGSm1gPO7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "PFKpzAaSgUjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "        .appName('04_pyspark') \\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "rVt1eyR1jHRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start a tunnel to access SparkUI\n",
        "\n",
        "Open a ngrok tunnel to the HTTP server"
      ],
      "metadata": {
        "id": "swvWh17c1Ump"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok, conf\n",
        "import getpass\n",
        "\n",
        "print(\"Enter your authtoken, which can be copied \"\n",
        "\"from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "\n",
        "ui_port = 4040\n",
        "public_url = ngrok.connect(ui_port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{ui_port}\\\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynYMotFwxGcX",
        "outputId": "d4f2e7cc-d0d3-44c4-a22c-571f8782e058"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\n",
            "··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-02-27T19:49:02+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"https://5882-34-106-31-34.ngrok-free.app\" -> \"http://127.0.0.1:4040\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.3.1 First Look at Spark and PySpark\n",
        "\n",
        "## Down fhv data and read into Spark\n",
        "\n",
        "Data source: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
      ],
      "metadata": {
        "id": "84401itCkpCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz"
      ],
      "metadata": {
        "id": "vZaeQd0va3h1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46fa3d3e-a424-4b7f-b8f2-bdb980762c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-27 19:36:32--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/035746e8-4e24-47e8-a3ce-edcf6d1b11c7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250227%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250227T193632Z&X-Amz-Expires=300&X-Amz-Signature=2b2af4bd6e614bbc715571c2e5ff3e65c21f72d4bca3c7d267dd9b9b9169decf&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-02-27 19:36:32--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/035746e8-4e24-47e8-a3ce-edcf6d1b11c7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250227%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250227T193632Z&X-Amz-Expires=300&X-Amz-Signature=2b2af4bd6e614bbc715571c2e5ff3e65c21f72d4bca3c7d267dd9b9b9169decf&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 129967421 (124M) [application/octet-stream]\n",
            "Saving to: ‘fhvhv_tripdata_2021-01.csv.gz’\n",
            "\n",
            "fhvhv_tripdata_2021 100%[===================>] 123.95M  32.1MB/s    in 3.8s    \n",
            "\n",
            "2025-02-27 19:36:36 (33.0 MB/s) - ‘fhvhv_tripdata_2021-01.csv.gz’ saved [129967421/129967421]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upzip the downloaded file\n",
        "!gzip -d fhvhv_tripdata_2021-01.csv.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJMR8mg25grX",
        "outputId": "cde7597a-cf2b-43bc-918c-571a0c87dde0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gzip: fhvhv_tripdata_2021-01.csv already exists; do you wish to overwrite (y or n)? y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count the number of lines in the csv file\n",
        "!wc -l fhvhv_tripdata_2021-01.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wsesZmG5j2j",
        "outputId": "96f7fd67-61fb-404f-917c-4772f881efa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11908469 fhvhv_tripdata_2021-01.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv('fhvhv_tripdata_2021-01.csv')"
      ],
      "metadata": {
        "id": "xU2eiCC16jof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all the values are strings. Spark does not infer the data types.\n",
        "df.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDWi5tXT6jmN",
        "outputId": "b43d0d9e-a39f-4824-baa5-fc590edf89e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('SR_Flag', StringType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Pandas to infer the data types and apply to Spark"
      ],
      "metadata": {
        "id": "KyigHqR-6z_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 1001 fhvhv_tripdata_2021-01.csv > head.csv"
      ],
      "metadata": {
        "id": "rVEEi0CW6jjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_pandas = pd.read_csv('head.csv')\n",
        "df_pandas.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "_9riGGou6jgn",
        "outputId": "4a82ba2c-c727-4b66-e254-4687be90815c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "hvfhs_license_num        object\n",
              "dispatching_base_num     object\n",
              "pickup_datetime          object\n",
              "dropoff_datetime         object\n",
              "PULocationID              int64\n",
              "DOLocationID              int64\n",
              "SR_Flag                 float64\n",
              "dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>hvfhs_license_num</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dispatching_base_num</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pickup_datetime</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dropoff_datetime</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PULocationID</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DOLocationID</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SR_Flag</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.createDataFrame(df_pandas).schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrlA1tRH6jd9",
        "outputId": "a7b49d6d-f17a-4ca4-c8e3-5dd579bd4e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('SR_Flag', DoubleType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import types"
      ],
      "metadata": {
        "id": "w5Twe9kw6jbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = types.StructType([\n",
        "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
        "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
        "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
        "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
        "    types.StructField('PULocationID', types.IntegerType(), True),\n",
        "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
        "    types.StructField('SR_Flag', types.StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "bVFU8jZJ6jXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(schema) \\\n",
        "    .csv('fhvhv_tripdata_2021-01.csv')"
      ],
      "metadata": {
        "id": "Pq_EWxwI6jQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partitions\n",
        "\n",
        "A Spark cluster is composed of multiple executors. Each executor can process data independently in order to parallelize and speed up work.\n",
        "\n",
        "In the previous example we read a single large CSV file. A file can only be read by a single executor, which means that the code we've written so far isn't parallelized and thus will only be run by a single executor rather than many at the same time.\n",
        "\n",
        "In order to solve this issue, we can split a file into multiple parts so that each executor can take care of a part and have all executors working simultaneously. These splits are called partitions."
      ],
      "metadata": {
        "id": "z1c7h3ll7aFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create 24 partitions in our dataframe (lazy execution: nothing happened in the Spark UI)\n",
        "df = df.repartition(24)\n",
        "# parquetize and write to fhvhv/2021/01/ folder\n",
        "df.write.parquet('fhvhv/2021/01/', mode='overwrite')"
      ],
      "metadata": {
        "id": "v9r4teNR7l2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.3.2 Spark DataFrames"
      ],
      "metadata": {
        "id": "QyAr_dV4-7sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike CSV files, parquet files contain the schema of the dataset, so there is no need to specify a schema like we previously did when reading the CSV file.\n",
        "\n",
        "(One of the reasons why parquet files are smaller than CSV files is because they store the data according to the datatypes, so integer values will take less space than long or string values.)"
      ],
      "metadata": {
        "id": "fPeJCppv_Q3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet('fhvhv/2021/01/')"
      ],
      "metadata": {
        "id": "OFOtxG4i7jf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-2_kIxb_KzF",
        "outputId": "b06f0595-7a9b-488b-b0d0-4717c4ae7a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- hvfhs_license_num: string (nullable = true)\n",
            " |-- dispatching_base_num: string (nullable = true)\n",
            " |-- pickup_datetime: timestamp (nullable = true)\n",
            " |-- dropoff_datetime: timestamp (nullable = true)\n",
            " |-- PULocationID: integer (nullable = true)\n",
            " |-- DOLocationID: integer (nullable = true)\n",
            " |-- SR_Flag: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actions vs Transformations\n",
        "\n",
        "Some Spark methods are \"lazy\", meaning that they are not executed right away. You can test this with the last instructions we run in the previous section: after running them, the Spark UI will not show any new jobs. However, running df.show() right after will execute right away and display the contents of the dataframe; the Spark UI will also show a new job.\n",
        "\n",
        "These lazy commands are called transformations and the eager commands are called actions. Computations only happen when actions are triggered."
      ],
      "metadata": {
        "id": "R9p6JsUVAD1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas-like operations"
      ],
      "metadata": {
        "id": "4dc0Qm5m_avV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID').filter(df.hvfhs_license_num == 'HV0003')"
      ],
      "metadata": {
        "id": "UUqSrzFV_Kwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzvDpo9Q_Ktw",
        "outputId": "7609c7b2-2d08-408f-a708-f3ea8f61df60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+------------+------------+\n",
            "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
            "+-------------------+-------------------+------------+------------+\n",
            "|2021-01-05 22:14:07|2021-01-05 22:32:28|         189|         107|\n",
            "|2021-01-02 17:59:55|2021-01-02 18:10:39|          88|         137|\n",
            "|2021-01-02 23:57:54|2021-01-03 00:15:48|         238|         224|\n",
            "|2021-01-06 15:53:13|2021-01-06 16:07:07|         169|         208|\n",
            "|2021-01-07 07:35:24|2021-01-07 07:55:49|          75|          88|\n",
            "|2021-01-07 08:45:12|2021-01-07 08:51:17|         210|         210|\n",
            "|2021-01-02 15:44:26|2021-01-02 16:10:50|         243|          69|\n",
            "|2021-01-04 16:50:28|2021-01-04 16:57:43|         250|         213|\n",
            "|2021-01-03 10:30:34|2021-01-03 10:44:53|          87|          79|\n",
            "|2021-01-03 22:05:20|2021-01-03 22:27:55|          68|         181|\n",
            "|2021-01-04 08:01:02|2021-01-04 08:33:27|          95|         236|\n",
            "|2021-01-02 13:01:10|2021-01-02 13:08:11|         262|         236|\n",
            "|2021-01-06 17:12:27|2021-01-06 17:46:56|         237|          83|\n",
            "|2021-01-04 09:05:18|2021-01-04 09:27:50|         159|          75|\n",
            "|2021-01-06 16:46:47|2021-01-06 17:50:24|         109|         119|\n",
            "|2021-01-06 08:03:47|2021-01-06 08:17:43|         145|         229|\n",
            "|2021-01-04 06:45:42|2021-01-04 06:55:01|         250|         212|\n",
            "|2021-01-03 13:20:41|2021-01-03 13:31:11|         130|          28|\n",
            "|2021-01-03 17:30:33|2021-01-03 17:45:19|          81|          46|\n",
            "|2021-01-06 20:55:57|2021-01-06 21:02:01|         113|          79|\n",
            "+-------------------+-------------------+------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions and User Defined Functions (UDFs)"
      ],
      "metadata": {
        "id": "NjIQcLwBAiB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "NqDXQxXN_Kq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df \\\n",
        "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
        "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
        "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzDvm5Qf_KoT",
        "outputId": "29bc396e-4372-4838-8074-f3edab39c6b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+------------+------------+\n",
            "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
            "+-----------+------------+------------+------------+\n",
            "| 2021-01-03|  2021-01-03|         255|          34|\n",
            "| 2021-01-05|  2021-01-05|         189|         107|\n",
            "| 2021-01-02|  2021-01-02|          88|         137|\n",
            "| 2021-01-02|  2021-01-03|         238|         224|\n",
            "| 2021-01-06|  2021-01-06|         169|         208|\n",
            "| 2021-01-07|  2021-01-07|          75|          88|\n",
            "| 2021-01-07|  2021-01-07|         210|         210|\n",
            "| 2021-01-02|  2021-01-02|         243|          69|\n",
            "| 2021-01-04|  2021-01-04|         250|         213|\n",
            "| 2021-01-03|  2021-01-03|          87|          79|\n",
            "| 2021-01-03|  2021-01-03|          68|         181|\n",
            "| 2021-01-04|  2021-01-04|          95|         236|\n",
            "| 2021-01-02|  2021-01-02|         262|         236|\n",
            "| 2021-01-04|  2021-01-04|         225|         233|\n",
            "| 2021-01-06|  2021-01-06|         237|          83|\n",
            "| 2021-01-05|  2021-01-05|         231|          87|\n",
            "| 2021-01-06|  2021-01-06|          22|          26|\n",
            "| 2021-01-04|  2021-01-04|         159|          75|\n",
            "| 2021-01-06|  2021-01-06|         109|         119|\n",
            "| 2021-01-06|  2021-01-06|         145|         229|\n",
            "+-----------+------------+------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A crazy function that changes values when they're divisible by 7 or 3\n",
        "def crazy_stuff(base_num):\n",
        "    num = int(base_num[1:])\n",
        "    if num % 7 == 0:\n",
        "        return f's/{num:03x}'\n",
        "    elif num % 3 == 0:\n",
        "        return f'a/{num:03x}'\n",
        "    else:\n",
        "        return f'e/{num:03x}'\n",
        "\n",
        "# Creating the actual UDF\n",
        "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())"
      ],
      "metadata": {
        "id": "B9FUW4wc_Klq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df \\\n",
        "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
        "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
        "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
        "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL-SuAdn_Ki_",
        "outputId": "fb545964-e1f8-456d-d9e2-1857e7332392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+------------+------------+------------+\n",
            "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
            "+-------+-----------+------------+------------+------------+\n",
            "|  e/9ce| 2021-01-03|  2021-01-03|         255|          34|\n",
            "|  e/b42| 2021-01-05|  2021-01-05|         189|         107|\n",
            "|  e/b33| 2021-01-02|  2021-01-02|          88|         137|\n",
            "|  e/b38| 2021-01-02|  2021-01-03|         238|         224|\n",
            "|  e/b3b| 2021-01-06|  2021-01-06|         169|         208|\n",
            "|  e/b33| 2021-01-07|  2021-01-07|          75|          88|\n",
            "|  e/acc| 2021-01-07|  2021-01-07|         210|         210|\n",
            "|  e/acc| 2021-01-02|  2021-01-02|         243|          69|\n",
            "|  e/b35| 2021-01-04|  2021-01-04|         250|         213|\n",
            "|  s/b3d| 2021-01-03|  2021-01-03|          87|          79|\n",
            "|  e/a39| 2021-01-03|  2021-01-03|          68|         181|\n",
            "|  s/acd| 2021-01-04|  2021-01-04|          95|         236|\n",
            "|  s/b13| 2021-01-02|  2021-01-02|         262|         236|\n",
            "|  e/9ce| 2021-01-04|  2021-01-04|         225|         233|\n",
            "|  e/b14| 2021-01-06|  2021-01-06|         237|          83|\n",
            "|  e/9ce| 2021-01-05|  2021-01-05|         231|          87|\n",
            "|  e/9ce| 2021-01-06|  2021-01-06|          22|          26|\n",
            "|  a/a7a| 2021-01-04|  2021-01-04|         159|          75|\n",
            "|  e/b35| 2021-01-06|  2021-01-06|         109|         119|\n",
            "|  a/b43| 2021-01-06|  2021-01-06|         145|         229|\n",
            "+-------+-----------+------------+------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MEP-3teC_KgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DISD3Xtb_Kdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c4k4_gb6_Ka_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k5gkaUua_KYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally put the tunnel down\n",
        "ngrok.disconnect(public_url)"
      ],
      "metadata": {
        "id": "5iluhZqWyIPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uMrdM-zjQ_Bb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}